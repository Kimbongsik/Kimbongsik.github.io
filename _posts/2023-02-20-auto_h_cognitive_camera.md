---
title: "[자율주행/h-mobility] 카메라 센서"
last_modified_at: 2023-02-20T16:20:02-05:00
toc: true
toc_sticky: true
toc_label: "목차"
categories:
  - auto

tags:
  - auto
---

# 개요

## 카메라 센서 원리

- 빛 → 렌즈를 통과 → 전기적 신호로 변환 : 2차원 배열 형태의 데이터 획득
- CCD(Charge Coupled Device)나 CMOS(Complementary Metal-Oxide-Semiconductior)이미지 센서에 영상 투사하여 전기적 신호를 획득함
- 카메라 구성
    - 렌즈(빛을 받아들임)
    - 몸체(상이 맺힘)
    - 조절 장치
        - 조리개
        - 셔터
- 카메라 센서 원리
    - 초점 거리: 렌즈의 중심에서부터 필름에 영상이 맺히는 사이의 거리. 즉, 이미지 센서와 렌즈와의 거리
    - 화각: 화면을 구성하는 각도
        - 광각렌즈: 초점거리가 짧으면 화각이 넓어짐
        - 초점거리가 길면 화각이 넓어짐
        - 화각에 따른 분류(초점거리가 짧아짐): 망원렌즈 < 표준렌즈 < 광각렌즈< 어안렌즈
        

## 카메라 센서 구조

- 핀홀 카메라
    - 아주 기본적인 카메라 구조
    - 초점거리에 따라 물체 크기 변화
    
    ![1](https://user-images.githubusercontent.com/63995044/220090759-1a065e07-f91e-4f5c-a0ef-974f2aec6c45.png)
    
- 렌즈 카메라
    - 렌즈의 빛의 굴절 특성 이용
    - 핀 홀 카메라의 이론을 그대로 사용 가능
        - ex: 초점 거리에 따른 물체 크기
    - 렌즈에 의한 왜곡 보정 필요

## 영상의 표현 방법

- 카메라 영상은 x축 y축의 2차원 배열로 표현
    - 배열의 크기. 배열에 몇 개의 원소가 있느냐: 해상도
    - 배열의 원소: 픽셀
- 해상도
    - HD(1280 * 720)
    - FHD(1290*1080)
    - UHD(3840*2160)
- 자율주행차는 시간적인 프레임을 위해 영상을 사용함
    - 프레임 비율(Frame Rate)
        - 1초 동안 보여주는 영상 프레임 수
    - 카메라 영상의 색상 표현
        - RGB 영상(적색, 녹색, 청색)
        - 일반적으로 각 색상의 밝기를 8비트(0~255)로 표현
        - YIQ, CMY, HIS 등 다양함
    

# Calibration(보정) 기술

## 카메라 좌표계

- 카메라는 3차원 공간에서 빛이 물체에 반사되어 들어오는 정보를 2차원 평면에 투영하여 영상을 생성함
- 월드 좌표계(실제 공간)
- 카메라 좌표계
    - 카메라 영상에서 물체를 표현하는 좌표계
    - 카메라의 초점이 0점
    
    ![2](https://user-images.githubusercontent.com/63995044/220090767-72ddaaca-c59c-40c9-9e40-f706c4a606e4.png)
    
    월드 좌표계 → 카메라 좌표계 변환
    

## 카메라 내부 파라미터

- 월드 좌표계 → 카메라 좌표계의 한 점으로 변환할 수 있는데, 이렇게 카메라 내부의 기계적인 세팅을 설명하는 것이 카메라 내부 파라미터.
- 대표 파라미터
    - 초점거리
        - 렌즈의 중심과 이미지 센서와의 거리
        - 픽셀 단위로 표현
    - 주점
        - 렌즈의 중심에서 이미지 센서에 수직으로 내린 점의 영상 좌표
        - 픽셀 단위로 표현
        - 반드시 영상의 중심에 해당하지 않기 때문에 이를 알아내야 좌표계 변환 가능
    - 비대칭 계수
        - 이미지 센서의 Cell Array의 Y축이 기울어진 정도
        - 요즘은 잘 안 씀
        

## 카메라 캘리브레이션

### [요약]

- 카메라 좌표계의 변환을 알기 위해 카메라의 내부 파라미터 값을 알아내고 카메라 렌즈에 의해 생긴 왜곡을 보정하는 과정
- 과정
    1. 반복적인 패턴이 있는 체커보드(체스판처럼 생김) 를 카메라로 촬영
    2. 영상에서 체커보드의 코너점들을 검출
    3. 결과를 이용하여 카메라의 내부 파라미터값 계산
- 자율주행을 위한 카메라 캘리브레이션
    - 자율주행차가 월드 좌표계에서의 물체 상태나 환경을 카메라 영상을 이용해 이해하기 위해서
    - 두 개 이상의 카메라 사용: 스테레오 카메라
        - 물체의 거리 정보를 알아낼 수 있음
        - 따라서 카메라 캘리브레이션이 매우 중요함

### [상세]

- **개요**
    - **월드 좌표계의 3차원 점과 대응하는 영상의 2차원 픽셀로의 변환 행렬 모델을 찾는 과정**
    - 이상적인 핀홀 카메라 행렬 모델
        - 내부 파라미터
            - 카메라 자체의 셋업에 관련된 파라미터
                - ex: 카메라 좌표계와 카메라 영상의 픽셀값과의 대응 설명
            - 영향을 미치는 것
                - 초점 거리
                - 렌즈 왜곡
                - 영상 중심값
                - 이미지 센서(Aspect Ratio, Skew Factor)
        - 외부 파라미터
            - 3차원 카메라 좌표계 → 3차원 월드 좌표계 변환과 관련된 파라미터
                - 회전 이동
                - 평행 이동
- **과정**
    - 3차원 월드 좌표계(1) → 3차원 카메라 좌표계(2) → 영상의 2차원 픽셀 좌표계(3)
        - (1)~(2): 외부 파라미터, (2)~(3): 내부 파라미터
    - 실제 렌즈 카메라
        - 렌즈의 왜곡을 보정하기 위한 캘리브레이션 또한 수행
        - 렌즈 왜곡
            - 방사형(Radial)
            - 접선형(Tangential)
    - 수행 과정
        - 체커보드와 같은 반복적 패턴을 카메라로 여러 장의 영상을 찍어 3D 월드 좌표계와 2D 이미지 픽셀의 대응점들을 찾아 수행
            1. 카메라 캘리브레이션 툴 사용
            2. 영상에서 체커보드의 코너점 파악
                - 실제 체커보드의 점과 영상에서 파악된 코너점과의 관계를 구함
            3. 외부 파라미터와 내부 파라미터 계산

# 카메라 기반 물체 검출/추적 기술

## 카메라 기반 물체 검출 기술

- 카메라 영상을 입력으로 받아 단일, 다중의 물체의 위치와 종류를 알아내는 기술
- 물체의 위치: 물체를 포함하는 2차원 바운딩 박스로 표현
- 카메라 영상 데이터는 2차원 배열 구조를 갖는 숫자 데이터에 불과함
    - 인공지능 기술(딥러닝) 도입
    - 물체 검출 기술의 성능이 크게 향상
- 딥러닝을 이용한 물체 검출 기술
    - **카메라 영상**에 **CNN**을 적용하여 물체의 **특징 추출**을 통한 검출
    1. CNN을 백본 네트워크로 사용하여 추상화된 특징 지도 추출
        - 백본 네트워크: 물체 검출을 위한 추상적인 특징을 네트워크로 사용
    2. 특징 지도의 각 원소마다 앵커 박스 이용
        - 앵커 박스: 각 원소마다의 기준 박스
    3. 앵커 박스의 상대 변위 추정, 물체 분류 수행
    - 과정(toggle)
        1. 많은 양의 카메라 영상 데이터의 물체 검출 정보 취득
        2. 직접 라벨링(Annotation): 사람들이 함
        3. 딥러닝 구조를 트레이닝
        4. 딥러닝 모델의 출력
        5. 물체를 포함하는 박스 좌표와 물체 분류에 대한 확률값 생성
            
            물체의 종류에 대한 0과 1 사이의 정수값
            
            ![3](https://user-images.githubusercontent.com/63995044/220090778-58ef1aef-cd85-48a5-918b-38b84952544b.png)
            
    
- 딥러닝 기반 물체 검출 방법(1단계/2단계 두 가지 방법이 있음)
    - 1단계 방법
        - 순차 과정
            1. 입력 데이터(RGB)
            2. 특징맵 추출(CNN)
            3. 검출 레이어(Detection Head)
            4. 결과
        - 특징
            - 신경망을 적용하여 물체에 대한 특징 추출 → 물체의 위치 & 종류 동시 판별
            - 알고리즘 예시: YOLO, SSD, RetinaNet
            - 간단한 구조로 인해 계산시간이 빠름
    
    - **2단계 방법(중요)**
        - **순차 과정**
            1. 입력 데이터(RGB)
            2. 특징맵 추출(CNN): 백본 네트워크
                - 카메라 영상에서 원하는 특징값 추출
                - VGGNet, ResNet 등의 CNN 모델 사용
                - ImageNet 데이터셋으로 미리 훈련된 가중치를 초기값으로 사용
            3. RPN(Region Proposal Network) : 물체의 존재 유무만 먼저 판별
                - 앵커 박스를 기준으로 하여 상대 변위 추정
            4. RoI Pooling: 물체가 존재한다고 생각되는 영역의 특징값 추출
                - RPN에서 출력된 박스를 이용해 백본 네트워크에서 얻은 특징 지도로부터 해당 영역의 특징값 추출
            5. Refinement Netowork(검출 레이어(Detection Head))
                - Rol Pooling을 통해 얻은 특징값을 이용해 물체의 정확한 종류 판별 및 위치 추정값 개선
            6. **결과**
            
    - **트레이닝 과정**
        - **물체의 바운딩 박스 라벨링**이 되어 있는 트레이닝 데이터 이용 → 손실함수가 최소가 되도록 **종단간(end-to-end) 학습** 수행
            - 손실함수의 지역적 최소값을 찾아가야 함
                - → Back Propagation 이용
        - **데이터 Augmentation 기법**
            - 학습해야 하는 파라미터 수에 비해 데이터의 개수가 모자랄 때, 트레이닝 데이터를 인공적으로 가공하여 추가적인 데이터셋을 생성함
            - 좌우대칭, 확대 및 축소, 밝기 조절, 회전, 노이즈 추가 등 추가적인 데이터셋을 삽입
            
    - **물체 검증기 성능 평가**
        - 정답 박스와 출력 박스 사이에 겹치는 면적(IoU, Intersection over Union)을 기반으로 판별
            - ex: 겹치는 면적 비율 > 0.5 : 참(TP)
        - Mean Average Precision(mAP)를 기준으로 판별
            - Precision
                - 물체라고 검출한 것 중 몇 개나 맞았는가?
                - ex: 사람 10명을 검출했는데 그 중 4명을 맞게 검출해냈다
            - Recall
                - 실제로 물체인 것 중에  몇 개나 검출했는가?
                - ex: 10개를 검출해야 하는데 5개만 맞다고 예측했다
            - Precision과 Recall은 서로 반대의 관계이며, 이러한 관계를(Precision-Recall 커브, PR 곡선) 적분한 값이 mAP 값
    - 특징(toggle)
        1. 물체의 존재 여부만을 검출
        2. 보다 정밀한 물체 위치 파악 & 물체 종류 인식
        
        → 두 번 거치기 때문에 2단계 방법이라고 함
        
        - 알고리즘 예시: Faster RCNN, Mask RCNN, Cascade R-CNN
        - 검출 정확도가 더 높음
        - 차량 주변의 동적 객체와 정적 객체를 동시에 검출
        - 환경 변화에 민감하기 때문에
            - 다양한 종류의 데이터 활용
            - 환경 변화에 적응적으로 동작하는 검출 기술 개발
                - ex: 밤, 우천 시 학습된 모델 적용
        - 임베디드 하드웨어에서 실시간으로 물체 검출 기술 필요
    

## 카메라 기반 물체 추적 기술

### 요약

- 자율주행차는 카메라 센서를 이용하여 각 카메라 영상의 순차적인 시퀀스로 구성된 비디오 데이터를 실시간으로 생성하게 됨
- 입력 영상 → 물체 검출 결과 이용 → 물체 추적
- 물체 추적 기술의 활용
    - 교통 상황 자동 분석
    - 무인 감시 카메라
    - 자율주행

### 과정(개요)

1. 현재 프레임에서의 물체 정보와 이전 프레임에서의 물체 정보 비교
2. 이전 프레임에서 추적 중인 검출 결과 예측
3. 2와 현재 프레임에서 나온 검출 결과  연결
    
    → 시간에 따른 물체의 움직임 추적
    
4. 정보 갱신
5. 같은 물체라고 판명이 되면 해당 물체에 ID 부여
6. 연결된 검출 결과의 필터링 및 예측
    - 트랙: 검출 결과를 유지하고 있는 물체의 시간적인 집합
    - 새로 등장, 퇴장하는 물체를 **연결**해 트랙에 추가
    - 칼만 필터, 딥러닝 모델 등을 통해 **필터링**
    - 다음 프레임 물체 위치 **예측**
    - 예측 결과를 다음 프레임 검출 결과에 연결
    
    → 검출 결과 연결 / 검출 결과 필터링 및 예측이 시간에 따라 순환하면서 수행
    

![4](https://user-images.githubusercontent.com/63995044/220090789-a0239a37-b239-43e9-adde-a56910dcba98.png)

![5](https://user-images.githubusercontent.com/63995044/220090801-3aa5590b-5909-4ab7-83d8-418c26bc9d98.png)

### 딥러닝 기술

- 과거 필터링 과정에서 칼만 필터를 사용하였으나, 인공지능 기술이 발달함에 따라 딥러닝 사용
1. **다음 프레임에서의 물체 필터링 및 예측을 위한 딥러닝**
    - 시간적 데이터를 처리하기 위한 딥러닝 모델 사용
2. **검출 결과 연결을 위한 딥러닝**
    - 과거 검출 결과와 현재 검출 결과의 유사성 측정
    - 딥뉴얼 네트워크를 통해 유사성의 정도 판단
    - 특징값, 박스 좌표, 비슷한 장소, 비슷한 외관 등 활용하여 유사성 측정
    
- **검출 결과 예측 방법**
    - T-1 프레임을 통해 T1 박스 좌표를 기준으로 하여 물체의 상대적 변위 예측이 가능해야 함
    
    → CNN 특징 지도 사용
    
    1. T 프레임의 영상을 CNN에 통과
    2. T-1 프레임의 박스 좌표를 기반으로 해당 영역의 특징값 추출
        - T-1 프레임 → T 프레임에서 박스 좌표의 변화 추정(딥러닝 사용)